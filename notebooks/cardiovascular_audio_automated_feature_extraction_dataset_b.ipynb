{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Heartbeat Sound Classification with Visual Domain Deep Neural Networks - Dataset B of Pascal heart sound classification challange</h1>\n",
    "\n",
    "Setting up libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqphPlnpdaGu",
    "outputId": "00df6f18-4f1a-45ec-b8be-0fa2637dbdc7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pydub\n",
    "!pip install --upgrade scikit-image\n",
    "!pip install librosa --user\n",
    "!pip install opencv-python\n",
    "! apt-get install libsndfile1-dev -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import neccesory libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onkwRu7zdtnW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from scipy.signal import butter,filtfilt\n",
    "import os,shutil\n",
    "import seaborn as sns\n",
    "from pydub import AudioSegment\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KmHiy5Rdy8p"
   },
   "outputs": [],
   "source": [
    "base_location=\"Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating neccesory folders, Add the file inside Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wusSocv7d4I1"
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"Data/set_b/spectograms/train_data\"\n",
    "!mkdir -p \"Data/set_b/spectograms/test_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "tWgL-LsEd9E5",
    "outputId": "fac4fd33-4ec3-4e79-852a-d6d0733ffc33"
   },
   "outputs": [],
   "source": [
    "setB=pd.read_csv(\"{0}/set_b.csv\".format(base_location))\n",
    "labelled_setB=setB[setB[\"label\"].notnull()]\n",
    "## removing the sublabel as thats not required for this study\n",
    "labelled_setB=labelled_setB[[\"dataset\",\"fname\",\"label\"]]\n",
    "sns.set(rc={'figure.figsize':(14,5)})\n",
    "ax = sns.countplot(x ='label', data = labelled_setB)\n",
    "ax.set(xlabel='Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Functions - Lowpass filter, includes denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsphIFvgeAgd"
   },
   "outputs": [],
   "source": [
    "def butter_lowpass_filter(audio_location, cutoff, order):\n",
    "    wav=get_wav(audio_location)\n",
    "    x,sr = librosa.load(audio_location,sr=wav.getframerate(),duration=(wav.getnframes()/wav.getframerate()))\n",
    "    nyq = 0.5 * sr\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Functions - get the audio duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gGJX43XeGiS"
   },
   "outputs": [],
   "source": [
    "def get_wav(audio_location):\n",
    "    wav = wave.open(audio_location)\n",
    "    return wav\n",
    "\n",
    "def get_audio_duration(audio_location):\n",
    "    wav = get_wav(audio_location)\n",
    "    return (wav.getnframes()/wav.getframerate())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSDnG_xLeJKW"
   },
   "outputs": [],
   "source": [
    "def get_audio_slice(start_time,end_time,audio_location,part):\n",
    "    start_ms=start_time*1000\n",
    "    end_ms=end_time*1000\n",
    "    newAudio = AudioSegment.from_wav(audio_location)\n",
    "    newAudio = newAudio[start_ms:end_ms]\n",
    "    current_file_loc=os.path.split(audio_location)\n",
    "    exported_file_loc=\"{0}/split/part_{1}_{2}\".format(current_file_loc[0],part,current_file_loc[1])\n",
    "    newAudio.export(exported_file_loc, format=\"wav\")\n",
    "    print (\"Successfully splitted the {0} as part {1} from {2} to {3} seconds, split file available in {4}\".format(current_file_loc[1], part, start_time,end_time, exported_file_loc))\n",
    "    return exported_file_loc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split audio files with a length of 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAjemegreLVs"
   },
   "outputs": [],
   "source": [
    "def split_files(audio_location):\n",
    "    part_factor=3\n",
    "    tot_duration=get_audio_duration(audio_location) \n",
    "    parts=int(tot_duration/part_factor)\n",
    "    part_file_list=[]\n",
    "    for part in range(parts):\n",
    "        start_time=part*part_factor\n",
    "        end_time=start_time+part_factor\n",
    "        #print (\"Start time of part {0} is {1} and end time is {2} \".format(part,start_time,end_time))  \n",
    "        file_loc=get_audio_slice(start_time,end_time,audio_location,part)\n",
    "        part_file_list.append(file_loc)\n",
    "\n",
    "    return part_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleansing the filenames and loading into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33PnsR3LeSyU"
   },
   "outputs": [],
   "source": [
    "labelled_setB=setB[setB[\"label\"].notnull()]\n",
    "labelled_setB=labelled_setB[[\"dataset\",\"fname\",\"label\"]]\n",
    "## Step 1 : remove \"Btraining_\" from the file name string for dataset B\n",
    "\n",
    "labelled_setB['fname'] = labelled_setB['fname'].map(lambda x : x.replace(\"Btraining_\",\"\"))\n",
    "\n",
    "## As the Audio files has double underscore in the file name for 1st occearnce, upding the index accordingly. Then _noisy file names also getting handled\n",
    "labelled_setB['fname'] = labelled_setB['fname'].map(lambda x : \"{0}/{1}\".format(x.split(\"/\")[0],x.split(\"/\")[1].replace(\"_\",\"__\",1)).replace(\"__noisy\",\"_noisy\"))\n",
    "\n",
    "# updating the duration\n",
    "\n",
    "labelled_setB['duration'] = labelled_setB['fname'].map(lambda x: get_audio_duration(\"{0}/{1}\".format(base_location,x) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting all files of dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASrAZKPMeZqe",
    "outputId": "cb524fe0-305c-49e3-c808-42f7bf657ab1"
   },
   "outputs": [],
   "source": [
    "labelled_setB['split_files']=labelled_setB['fname'].map(lambda x: split_files(\"{0}/{1}\".format(base_location,x) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening the base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmHeonepeZgJ",
    "outputId": "d3f0f669-f301-4ad2-d81d-f8ada5fb2488"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "flatdata = pd.DataFrame([( index, value) for ( index, values)\n",
    "                         in labelled_setB[ 'split_files' ].iteritems() for value in values],\n",
    "                             columns = [ 'index', 'split_files']).set_index( 'index' )\n",
    "  \n",
    "flattened_labelled_setB = labelled_setB.drop( 'split_files', axis = 1 ).join( flatdata )\n",
    "#display(flattened_labelled_setB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the flattened dataframe - This will show the data distribution of split files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "KBad_va0eZPh",
    "outputId": "5a185c80-2c6a-4378-8ac9-909abc3cc08c"
   },
   "outputs": [],
   "source": [
    "## Removing files less than 3 seconds from the index\n",
    "flattened_labelled_setB=flattened_labelled_setB[flattened_labelled_setB['split_files'].notnull()]\n",
    "sns.set(rc={'figure.figsize':(14,5)})\n",
    "ax = sns.countplot(x ='label', data = flattened_labelled_setB, palette=[\"#ab594f\",\"#d19421\",\"green\"])\n",
    "\n",
    "count = flattened_labelled_setB.groupby(['label'])['fname'].count().values\n",
    "\n",
    "pos = range(len(count))\n",
    "\n",
    "\n",
    "for tick in pos:\n",
    "  ax.text(pos[tick],count[tick]+20, count[tick], horizontalalignment='center', size='small', color='black', weight='regular')\n",
    "ax.set(xlabel='Cardiovascular sound category')\n",
    "ax.set(ylabel='Count')\n",
    "ax.set(title=\"Dataset B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the denoising cutoff and denoising the audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zODNqSXvkiTc"
   },
   "outputs": [],
   "source": [
    "cutoff=192 #Hz\n",
    "order=1\n",
    "#butter_lowpass_filter(str(x),cutoff,order)\n",
    "flattened_labelled_setB[\"denoised_signal\"]=flattened_labelled_setB['split_files'].map(lambda x: butter_lowpass_filter(str(x),cutoff,order) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFwEVRhUgIG9"
   },
   "outputs": [],
   "source": [
    "flattened_labelled_setB.to_csv(\"Data/set_b/denoise_split_master.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffling the denoised audio signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7f7HWc7gJHX"
   },
   "outputs": [],
   "source": [
    "flattened_labelled_setB=shuffle(flattened_labelled_setB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "PpEtkqkFjgTY",
    "outputId": "f532ecc7-089c-4abd-be41-11001d910283"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(flattened_labelled_setB, test_size=0.3)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "## Removing files less than 3 seconds from the index\n",
    "fig, ax = plt.subplots(1,2, sharey=True,figsize=(15, 5))\n",
    "#sns.set(rc={'figure.figsize':(14,5)})\n",
    "ax_1 = sns.countplot(x ='label', data = train,ax=ax[0])\n",
    "ax_1.set(xlabel='TRAIN')\n",
    "ax_2 = sns.countplot(x ='label', data = test,ax=ax[1])\n",
    "ax_2.set(xlabel='TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of specAugment - reference https://github.com/KimJeongSun/SpecAugment_numpy_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QshIXvyJjj-3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as nl\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import dct,idct\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "import skimage.io\n",
    "\n",
    "\n",
    "def plot_spec(spec,out):\n",
    "    librosa.display.specshow(spec, fmax=8000)\n",
    "    plt.savefig(out)\n",
    "    plt.cla()\n",
    "\n",
    "\n",
    "def makeT(cp):\n",
    "    # cp: [K x 2] control points\n",
    "    # T: [(K+3) x (K+3)]\n",
    "    K = cp.shape[0]\n",
    "    T = np.zeros((K+3, K+3))\n",
    "    T[:K, 0] = 1\n",
    "    T[:K, 1:3] = cp\n",
    "    T[K, 3:] = 1\n",
    "    T[K+1:, 3:] = cp.T\n",
    "    R = squareform(pdist(cp, metric='euclidean'))\n",
    "    R = R * R\n",
    "    R[R == 0] = 1 # a trick to make R ln(R) 0\n",
    "    R = R * np.log(R)\n",
    "    np.fill_diagonal(R, 0)\n",
    "    T[:K, 3:] = R\n",
    "    return T\n",
    "\n",
    "def liftPts(p, cp):\n",
    "    # p: [N x 2], input points\n",
    "    # cp: [K x 2], control points\n",
    "    # pLift: [N x (3+K)], lifted input points\n",
    "    N, K = p.shape[0], cp.shape[0]\n",
    "    pLift = np.zeros((N, K+3))\n",
    "    pLift[:,0] = 1\n",
    "    pLift[:,1:3] = p\n",
    "    R = cdist(p, cp, 'euclidean')\n",
    "    R = R * R\n",
    "    R[R == 0] = 1\n",
    "    R = R * np.log(R)\n",
    "    pLift[:,3:] = R\n",
    "    return pLift\n",
    "    \n",
    "def specAug(audio,sampling_rate,num,out):\n",
    "    time_sum = 0\n",
    "\n",
    "    #audio, sampling_rate = librosa.load(args.inpu,sr=t)\n",
    "    spec = librosa.feature.melspectrogram(y=audio,sr=sampling_rate,n_fft=200, hop_length=4)\n",
    "    spec = librosa.power_to_db(spec,ref=np.max)\n",
    "    \n",
    "    print(\"start to SpecAugment %d times\" % num)\n",
    "    for n in range(num): \n",
    "        start = time.time()\n",
    "        W=40\n",
    "        T=30\n",
    "        F=13\n",
    "        mt=2\n",
    "        mf=2\n",
    "\n",
    "        # Nframe : number of spectrum frame\n",
    "        Nframe = spec.shape[1]\n",
    "        # Nbin : number of spectrum freq bin\n",
    "        Nbin = spec.shape[0]\n",
    "        # check input length\n",
    "        if Nframe < W*2+1:\n",
    "            W = int(Nframe/4)\n",
    "        if Nframe < T*2+1:\n",
    "            T = int(Nframe/mt)\n",
    "        if Nbin < F*2+1:\n",
    "            F = int(Nbin/mf)\n",
    "\n",
    "        # warping parameter initialize\n",
    "        w = random.randint(-W,W)\n",
    "        center = random.randint(W,Nframe-W)\n",
    "\n",
    "        src = np.asarray([[ float(center),  1], [ float(center),  0], [ float(center),  2], [0, 0], [0, 1], [0, 2], [Nframe-1, 0], [Nframe-1, 1], [Nframe-1, 2]])\n",
    "        dst = np.asarray([[ float(center+w),  1], [ float(center+w),  0], [ float(center+w),  2], [0, 0], [0, 1], [0, 2], [Nframe-1, 0], [Nframe-1, 1], [Nframe-1, 2]])\n",
    "        #print(src,dst)\n",
    "\n",
    "        # source control points\n",
    "        xs, ys = src[:,0],src[:,1]\n",
    "        cps = np.vstack([xs, ys]).T\n",
    "        # target control points\n",
    "        xt, yt = dst[:,0],dst[:,1]\n",
    "        # construct TT\n",
    "        TT = makeT(cps)\n",
    "\n",
    "        # solve cx, cy (coefficients for x and y)\n",
    "        xtAug = np.concatenate([xt, np.zeros(3)])\n",
    "        ytAug = np.concatenate([yt, np.zeros(3)])\n",
    "        cx = nl.solve(TT, xtAug) # [K+3]\n",
    "        cy = nl.solve(TT, ytAug)\n",
    "\n",
    "        # dense grid\n",
    "        x = np.linspace(0, Nframe-1,Nframe)\n",
    "        y = np.linspace(1,1,1)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "\n",
    "        xgs, ygs = x.flatten(), y.flatten()\n",
    "\n",
    "        gps = np.vstack([xgs, ygs]).T\n",
    "\n",
    "        # transform\n",
    "        pgLift = liftPts(gps, cps) # [N x (K+3)]\n",
    "        xgt = np.dot(pgLift, cx.T)     \n",
    "        spec_warped = np.zeros_like(spec)\n",
    "        for f_ind in range(Nbin):\n",
    "            spec_tmp = spec[f_ind,:]\n",
    "            func = interpolate.interp1d(xgt, spec_tmp,fill_value=\"extrapolate\")\n",
    "            xnew = np.linspace(0, Nframe-1,Nframe)\n",
    "            spec_warped[f_ind,:] = func(xnew)\n",
    "\n",
    "        # sample mt of time mask ranges\n",
    "        t = np.random.randint(T-1, size=mt)+1\n",
    "        # sample mf of freq mask ranges\n",
    "        f = np.random.randint(F-1, size=mf)+1\n",
    "        # mask_t : time mask vector\n",
    "        mask_t = np.ones((Nframe,1))\n",
    "        ind = 0\n",
    "        t_tmp = t.sum() + mt\n",
    "        for _t in t:\n",
    "            k = random.randint(ind,Nframe-t_tmp)\n",
    "            mask_t[k:k+_t] = 0\n",
    "            ind = k+_t+1\n",
    "            t_tmp = t_tmp - (_t+1)\n",
    "        mask_t[ind:] = 1\n",
    "\n",
    "        # mask_f : freq mask vector\n",
    "        mask_f = np.ones((Nbin,1))\n",
    "        ind = 0\n",
    "        f_tmp = f.sum() + mf\n",
    "        for _f in f:\n",
    "            k = random.randint(ind,Nbin-f_tmp)\n",
    "            mask_f[k:k+_f] = 0\n",
    "            ind = k+_f+1\n",
    "            f_tmp = f_tmp - (_f+1)\n",
    "        mask_f[ind:] = 1\n",
    "\n",
    "        # calculate mean\n",
    "        mean = np.mean(spec_warped)\n",
    "\n",
    "        # make spectrum to zero mean\n",
    "        spec_zero = spec_warped-mean\n",
    "\n",
    "        spec_masked = ((spec_zero * mask_t.T) * mask_f) + mean\n",
    "    #     spec_masked = ((spec_zero * mask_t).T * mask_f).T\n",
    "\n",
    "        end = time.time()\n",
    "        time_sum += (end - start)  \n",
    "        if n == 0:\n",
    "          plot_spec(spec,\"{0}_orginal.png\".format(out))\n",
    "          plot_spec(spec_warped,\"{0}_wrapped.png\".format(out))\n",
    "        plot_spec(spec_masked,\"{0}_masked_{1}.png\".format(out,n))\n",
    "    print(\"whole processing time : %.4f second\" % (time_sum))   \n",
    "    print(\"average processing time : %.2f ms\" % (time_sum*1000/num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Spectrogram and Augment with timewrapped and frequency masked Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WOZ1ADgjsPD"
   },
   "outputs": [],
   "source": [
    "def augment_spectrograms(audio,sr,filename,aug_num,step):\n",
    "  file_list=[]\n",
    "  filename=filename.replace(\"split\",\"spectograms/{0}_data\".format(step))\n",
    "  filename=filename.replace(\".wav\",\"\")\n",
    "  specAug(audio,sr,aug_num,filename)\n",
    "  file_list.append(\"{0}_orginal.png\".format(filename))\n",
    "  if step =='train':\n",
    "    file_list.append(\"{0}_wrapped.png\".format(filename))\n",
    "    for n in range (aug_num):\n",
    "        file_list.append(\"{0}_masked_{1}.png\".format(filename,n))   \n",
    "  return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating training spectogram images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sWsFEspXkBUV",
    "outputId": "3f717989-9c88-473d-ba10-aa33cc355f0b"
   },
   "outputs": [],
   "source": [
    "sr=4000\n",
    "data=list((map(lambda x,y: augment_spectrograms(x,sr,y,1,'train'),train['denoised_signal'],train['split_files'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating only the test spectograms (without augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y2WD5yC5kJK9",
    "outputId": "a001e92a-b0e8-4c55-dc1f-0709fa8ee7e1"
   },
   "outputs": [],
   "source": [
    "test_data=list((map(lambda x,y: augment_spectrograms(x,sr,y,1,'test'),test['denoised_signal'],test['split_files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding file path information into existing pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z54Fk-tLmMFM"
   },
   "outputs": [],
   "source": [
    "train=train.assign(augmented_files = data)\n",
    "test=test.assign(augmented_files = test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5WA54xblxUjv",
    "outputId": "dfcacc12-7149-42ac-c147-602410c35103"
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening both train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ6uvOA4wFdP"
   },
   "outputs": [],
   "source": [
    "flat_aug_data = pd.DataFrame([( index, value) for ( index, values)\n",
    "                         in train[ 'augmented_files' ].iteritems() for value in values],\n",
    "                             columns = [ 'index', 'augmented_files']).set_index( 'index' )\n",
    "  \n",
    "train_aug = train.drop( 'augmented_files', axis = 1 ).join( flat_aug_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "notuaVYbwMIO"
   },
   "outputs": [],
   "source": [
    "flat_aug_data = pd.DataFrame([( index, value) for ( index, values)\n",
    "                         in test[ 'augmented_files' ].iteritems() for value in values],\n",
    "                             columns = [ 'index', 'augmented_files']).set_index( 'index' )\n",
    "  \n",
    "test_aug = test.drop( 'augmented_files', axis = 1 ).join( flat_aug_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the train and test file location and label information. Also visualzing the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "lvZ3daM-wWo3",
    "outputId": "b20b26c7-de64-44fe-b51d-fb39e680a2f9"
   },
   "outputs": [],
   "source": [
    "train_aug_file_label=train_aug[[\"augmented_files\",\"label\"]]\n",
    "train_aug_file_label.to_csv(\"set_b_train_file_to_label.csv\")\n",
    "ax_1 = sns.countplot(x ='label', data = train_aug_file_label)\n",
    "ax_1.set(xlabel='TRAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "tSkOKqPzwkni",
    "outputId": "92bd45e1-819e-47af-fb08-8a244edfcafd"
   },
   "outputs": [],
   "source": [
    "test_aug_file_label=test_aug[[\"augmented_files\",\"label\"]]\n",
    "test_aug_file_label.to_csv(\"set_b_test_file_to_label_v1.csv\")\n",
    "ax_1 = sns.countplot(x ='label', data = test_aug_file_label)\n",
    "ax_1.set(xlabel='TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvcUFAxIxA-W"
   },
   "outputs": [],
   "source": [
    "# function to plot n images using subplots\n",
    "import cv2 as cv\n",
    "from skimage.transform import rescale, resize\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "        \n",
    "def plot_image(images, captions=None, cmap=None ):\n",
    "    f, axes = plt.subplots(1, len(images), sharey=True)\n",
    "    f.set_figwidth(15)\n",
    "    for ax,image in zip(axes,images):\n",
    "        ax.imshow(image, cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7m6aPYofyoWd"
   },
   "outputs": [],
   "source": [
    "def image_scale(imagePath, image_height, image_width):\n",
    "    image = cv.imread(imagePath)\n",
    "    image = cv.resize(image, (image_height, image_width))                    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing spectrograms into 128*128 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwk1eeaWyrKC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_mapping=pd.read_csv(\"set_b_train_file_to_label.csv\")\n",
    "data_mapping[\"image\"]=data_mapping['augmented_files'].map(lambda x : image_scale(x,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling dataset and Normalzing pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J03GRVuy2MT"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "#=data_mapping['image'].map(lambda x : x/255.0)\n",
    "data_mapping=shuffle(data_mapping)\n",
    "data=np.array(list(data_mapping[\"image\"]), dtype=\"float\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding labales as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDENMZXDy9IZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "le = LabelEncoder()\n",
    "labels=list(data_mapping[\"label\"])\n",
    "\n",
    "train_label = le.fit_transform(labels)\n",
    "labels = to_categorical(train_label,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting each category for weight calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMBEO32LzCRt",
    "outputId": "96bafea9-5a57-4284-9cc4-86fa103be8de"
   },
   "outputs": [],
   "source": [
    "murmur= len(list(filter(lambda x : x==1 , train_label)))\n",
    "normal= len(list(filter(lambda x : x==2 , train_label)))\n",
    "extrasystole=len(list(filter(lambda x : x==0 , train_label)))\n",
    "extrasystole+normal+murmur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk7arBXRLjQX",
    "outputId": "c226bb50-117a-4a6a-d988-750fbbafc057"
   },
   "outputs": [],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training vs Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyldezfDzHvq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Image generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iNzSJ23zNXV"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator()\n",
    "val_aug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading test data for evaluation and pre-prprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itjRHP7nzcpY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import librosa\n",
    "#import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from scipy.signal import butter,filtfilt\n",
    "import os,shutil\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cutoff=192 #Hz\n",
    "order=1\n",
    "#butter_lowpass_filter(str(x),cutoff,order)\n",
    "\n",
    "import pandas as pd\n",
    "test_data_mapping=pd.read_csv(\"\"\"set_b_test_file_to_label_v1.csv\"\"\")\n",
    "test_data_mapping[\"image\"]=test_data_mapping['augmented_files'].map(lambda x : image_scale(x,128,128))\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "#=data_mapping['image'].map(lambda x : x/255.0)\n",
    "test_data_mapping=shuffle(test_data_mapping)\n",
    "test_data=np.array(list(test_data_mapping[\"image\"]), dtype=\"float\") / 255.0\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "#le = LabelEncoder()\n",
    "test_labels=list(test_data_mapping[\"label\"])\n",
    "\n",
    "test_labels = le.transform(test_labels)\n",
    "test_labels = to_categorical(test_labels,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callback(model_type, reduce_plateau_factor, patience_val,save_best_only):\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "    model_name = 'models/'\n",
    "    \n",
    "    if not os.path.exists(model_name):\n",
    "         os.mkdir(model_name)\n",
    "        \n",
    "    filepath = model_name + model_type+ '-model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=save_best_only, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "    LR = ReduceLROnPlateau(monitor='val_loss', factor=reduce_plateau_factor, patience=patience_val, verbose = 1)\n",
    "    callbacks_list = [checkpoint, LR]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16, MobileNet,ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout,LSTM, Reshape, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "#from keras.metrics import TruePositives\n",
    "#from tensorflow.keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "\n",
    "lr=0.01; momentum_val=0.9;\n",
    "EPOCHS=20\n",
    "BS=8\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "#opt = SGD(lr=lr, momentum=0.9, decay=lr / EPOCHS)\n",
    "opt = Adam(lr=lr, decay=lr / EPOCHS)\n",
    "pretrained_model = MobileNet (\n",
    "        include_top=False,\n",
    "        input_shape=(128,128,3),\n",
    "        weights='imagenet'\n",
    "     )\n",
    "#pretrained_model.trainable = True\n",
    "\n",
    "\n",
    "train_generator = aug.flow(trainX, trainY, batch_size=BS)\n",
    "\n",
    "validation_generator = val_aug.flow(testX, testY,batch_size=BS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(pretrained_model)\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = \"relu\")) # fully connected\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "weight_for_extrasystole = (extrasystole)/(train_label.size) \n",
    "weight_for_murmur = (murmur)/(train_label.size) \n",
    "weight_for_normal = (normal)/(train_label.size) \n",
    "\n",
    "class_weight = {0: weight_for_extrasystole, 1: weight_for_murmur, 2: weight_for_normal}\n",
    "\n",
    "\n",
    "model_type=\"MobileNet\" ; reduce_plateau_factor=0.2; patience_val=5\n",
    "callbacks_list = create_callback(model_type, reduce_plateau_factor, patience_val, save_best_only = True)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"categorical_accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),tf.keras.metrics.AUC()])\n",
    "# train the network\n",
    "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=BS),steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "                        validation_data=(testX, testY),class_weight=class_weight,callbacks=callbacks_list,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation with MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"models/MobileNet-model-00015-0.02524-0.92047-0.22122-0.93095.h5\")\n",
    "\n",
    "\n",
    "score, acc,precision,recall,auc = model.evaluate(test_data, test_labels, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16, MobileNet,ResNet50,InceptionResNetV2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout,LSTM, Reshape, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    " \n",
    "\n",
    "lr=0.001; momentum_val=0.9;\n",
    "EPOCHS=10\n",
    "BS=8\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=lr, momentum=0.9, decay=lr / EPOCHS)\n",
    "#opt = Adam(lr=lr, decay=lr / EPOCHS)\n",
    "pretrained_model = InceptionResNetV2 (\n",
    "        include_top=False,\n",
    "        input_shape=(128,128,3),\n",
    "        weights='imagenet'\n",
    "     )\n",
    "#pretrained_model.trainable = True\n",
    "\n",
    "\n",
    "train_generator = aug.flow(trainX, trainY, batch_size=BS)\n",
    "\n",
    "validation_generator = val_aug.flow(testX, testY,batch_size=BS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(pretrained_model)\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = \"relu\")) # fully connected\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "weight_for_extrasystole = (extrasystole)/(train_label.size) \n",
    "weight_for_murmur = (murmur)/(train_label.size) \n",
    "weight_for_normal = (normal)/(train_label.size) \n",
    "\n",
    "class_weight = {0: weight_for_extrasystole, 1: weight_for_murmur, 2: weight_for_normal}\n",
    "\n",
    "\n",
    "model_type=\"InceptionResNetV2\" ; reduce_plateau_factor=0.2; patience_val=5\n",
    "callbacks_list = create_callback(model_type, reduce_plateau_factor, patience_val, save_best_only = True)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"categorical_accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),tf.keras.metrics.AUC()])\n",
    "# train the network\n",
    "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=BS),steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "                        validation_data=(testX, testY),class_weight=class_weight,callbacks=callbacks_list,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eveluation with InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"models/InceptionResNetV2-model-00005-0.01576-0.95970-0.14147-0.96905.h5\")\n",
    "\n",
    "test_aug = ImageDataGenerator()\n",
    "\n",
    "\n",
    "score, acc,precision,recall,auc = model.evaluate(test_data, test_labels, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16, MobileNet,ResNet50,InceptionResNetV2,MobileNetV2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout,LSTM, Reshape, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "#from keras.metrics import TruePositives\n",
    "#from tensorflow.keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "\n",
    "lr=0.0001; momentum_val=0.9;\n",
    "EPOCHS=10\n",
    "BS=8\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=lr, momentum=0.9, decay=lr / EPOCHS)\n",
    "#opt = Adam(lr=lr, decay=lr / EPOCHS)\n",
    "pretrained_model = MobileNetV2 (\n",
    "        include_top=False,\n",
    "        input_shape=(128,128,3),\n",
    "        weights='imagenet'\n",
    "     )\n",
    "#pretrained_model.trainable = True\n",
    "\n",
    "\n",
    "train_generator = aug.flow(trainX, trainY, batch_size=BS)\n",
    "\n",
    "validation_generator = val_aug.flow(testX, testY,batch_size=BS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(pretrained_model)\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = \"relu\")) # fully connected\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "weight_for_extrasystole = (extrasystole)/(train_label.size) \n",
    "weight_for_murmur = (murmur)/(train_label.size) \n",
    "weight_for_normal = (normal)/(train_label.size) \n",
    "\n",
    "class_weight = {0: weight_for_extrasystole, 1: weight_for_murmur, 2: weight_for_normal}\n",
    "\n",
    "\n",
    "model_type=\"MobileNetV2\" ; reduce_plateau_factor=0.2; patience_val=5\n",
    "callbacks_list = create_callback(model_type, reduce_plateau_factor, patience_val, save_best_only = True)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"categorical_accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),tf.keras.metrics.AUC()])\n",
    "# train the network\n",
    "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=BS),steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "                        validation_data=(testX, testY),class_weight=class_weight,callbacks=callbacks_list,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"models/MobileNetV2-model-00007-0.02528-0.93401-0.29812-0.90041.h5\")\n",
    "\n",
    "test_aug = ImageDataGenerator()\n",
    "\n",
    "\n",
    "score, acc,precision,recall,auc = model.evaluate(test_data, test_labels, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, sharey=True,figsize=(15, 5))\n",
    "\n",
    "plt.plot(H.history['auc_3'])\n",
    "plt.plot(H.history['val_auc_3'])\n",
    "plt.title('Dataset B - Training History - MobileNetV2')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16, MobileNet,ResNet50,InceptionResNetV2,MobileNetV2,EfficientNetB5,ResNet152V2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout,LSTM, Reshape, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "\n",
    "lr=0.001; momentum_val=0.9;\n",
    "EPOCHS=20\n",
    "BS=8\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "#opt = SGD(lr=lr, momentum=0.9, decay=lr / EPOCHS)\n",
    "opt = Adam(lr=lr, decay=lr / EPOCHS)\n",
    "pretrained_model = ResNet152V2 (\n",
    "        include_top=False,\n",
    "        input_shape=(128,128,3),\n",
    "        weights='imagenet'\n",
    "     )\n",
    "#pretrained_model.trainable = True\n",
    "\n",
    "\n",
    "train_generator = aug.flow(trainX, trainY, batch_size=BS)\n",
    "\n",
    "validation_generator = val_aug.flow(testX, testY,batch_size=BS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(pretrained_model)\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = \"relu\")) # fully connected\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "weight_for_extrasystole = (extrasystole)/(train_label.size) \n",
    "weight_for_murmur = (murmur)/(train_label.size) \n",
    "weight_for_normal = (normal)/(train_label.size) \n",
    "\n",
    "class_weight = {0: weight_for_extrasystole, 1: weight_for_murmur, 2: weight_for_normal}\n",
    "\n",
    "\n",
    "model_type=\"ResNet152V2\" ; reduce_plateau_factor=0.2; patience_val=5\n",
    "callbacks_list = create_callback(model_type, reduce_plateau_factor, patience_val, save_best_only = True)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"categorical_accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),tf.keras.metrics.AUC()])\n",
    "# train the network\n",
    "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=BS),steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "                        validation_data=(testX, testY),class_weight=class_weight,callbacks=callbacks_list,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, sharey=True,figsize=(15, 5))\n",
    "\n",
    "plt.plot(H.history['auc_5'])\n",
    "plt.plot(H.history['val_auc_5'])\n",
    "plt.title('Dataset B - Training History - ResNet152V2')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet152V2 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"models/ResNet152V2-model-00017-0.01505-0.94254-0.21193-0.94074.h5\")\n",
    "\n",
    "score, acc,precision,recall,auc = model.evaluate(test_data, test_labels, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16, MobileNet,ResNet50,InceptionResNetV2,MobileNetV2,EfficientNetB5,ResNet152V2,Xception\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout,LSTM, Reshape, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "\n",
    "lr=0.001; momentum_val=0.9;\n",
    "EPOCHS=7\n",
    "BS=8\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "#opt = SGD(lr=lr, momentum=0.9, decay=lr / EPOCHS)\n",
    "opt = Adam(lr=lr, decay=lr / EPOCHS)\n",
    "pretrained_model = Xception (\n",
    "        include_top=False,\n",
    "        input_shape=(128,128,3),\n",
    "        weights='imagenet'\n",
    "     )\n",
    "\n",
    "\n",
    "train_generator = aug.flow(trainX, trainY, batch_size=BS)\n",
    "\n",
    "validation_generator = val_aug.flow(testX, testY,batch_size=BS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(pretrained_model)\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = \"relu\")) # fully connected\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "weight_for_extrasystole = (extrasystole)/(train_label.size) \n",
    "weight_for_murmur = (murmur)/(train_label.size) \n",
    "weight_for_normal = (normal)/(train_label.size) \n",
    "\n",
    "class_weight = {0: weight_for_extrasystole, 1: weight_for_murmur, 2: weight_for_normal}\n",
    "\n",
    "\n",
    "model_type=\"Xception\" ; reduce_plateau_factor=0.2; patience_val=5\n",
    "callbacks_list = create_callback(model_type, reduce_plateau_factor, patience_val, save_best_only = True)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"categorical_accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),tf.keras.metrics.AUC()])\n",
    "# train the network\n",
    "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=BS),steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "                        validation_data=(testX, testY),class_weight=class_weight,callbacks=callbacks_list,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation with Xception Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"models/Xception-model-00005-0.02759-0.92100-0.18878-0.92222.h5\")\n",
    "\n",
    "\n",
    "score, acc,precision,recall,auc = model.evaluate(test_data, test_labels, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cardio_vascular_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
